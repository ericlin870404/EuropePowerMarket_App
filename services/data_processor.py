# services/data_processor.py

"""
ğŸ“Œ æ•´é«”æµç¨‹ï¼š
1. å¼•å…¥å¿…è¦å¥—ä»¶èˆ‡è¨­å®š
2. å®šç¾©è¼”åŠ©å·¥å…· (è§£æåº¦è¨ˆç®—ã€è£œå€¼é‚è¼¯)
3. å®šç¾©ä¸»åŠŸèƒ½ï¼šXML è½‰ Raw CSV (parse_da_xml_to_raw_csv_bytes)
   3-1. è®€å– XML ä¸¦éæ¿¾ classificationSequence
   3-2. é€é Helper å–å¾—äº¤å‰²æ—¥ä¸¦å»é‡
   3-3. è§£æ Point ä¸¦ä¾ ENTSO-E è¦å‰‡è£œå€¼
   3-4. è¼¸å‡º Raw CSV Bytes
4. å®šç¾©ä¸»åŠŸèƒ½ï¼šRaw CSV è½‰ Hourly CSV (convert_raw_mtu_csv_to_hourly_csv_bytes)
   4-1. è®€å– Raw CSV ä¸¦æª¢æŸ¥æ¬„ä½
   4-2. ä¾ settings è¨­å®šæª¢æŸ¥æ¯æ—¥è³‡æ–™ç­†æ•¸ (24/48/96)
   4-3. èšåˆé‹ç®— (å¹³å‡å€¼) è½‰æ›ç‚ºå°æ™‚è³‡æ–™
   4-4. è¼¸å‡º Hourly CSV Bytes
"""

from __future__ import annotations

import io
import xml.etree.ElementTree as ET
from datetime import date
from typing import List, Tuple, Set

import pandas as pd

from config.settings import (
    DA_SUPPORTED_RESOLUTION_MINUTES,
    DA_SKIP_UNSUPPORTED_MTU_DAYS,
)
from utils.timezone_helper import get_da_delivery_date_from_timeseries


# =========================== #
# 2 ğŸ”¹ å®šç¾©è¼”åŠ©å·¥å…·
# =========================== #
def _resolution_to_expected_points(resolution: str) -> int:
    """
    å°‡ 'PT60M' ç­‰è§£ææˆä¸€å¤©æ‡‰æœ‰çš„é»æ•¸ã€‚
    ç›®å‰å‡è¨­ resolution çš†ç‚ºã€Œä»¥åˆ†é˜ç‚ºå–®ä½ã€ï¼š
    - PT60M â†’ 24
    - PT30M â†’ 48
    - PT15M â†’ 96
    """
    res = resolution.strip().upper()
    if not res.startswith("PT") or not res.endswith("M"):
        raise ValueError(f"æš«ä¸æ”¯æ´çš„ resolution æ ¼å¼ï¼š{resolution}")

    minutes_str = res[2:-1]  # å»æ‰ 'PT' å’Œ 'M'
    try:
        minutes = int(minutes_str)
    except ValueError:
        raise ValueError(f"è§£æ resolution åˆ†é˜æ•¸å¤±æ•—ï¼š{resolution}")

    if minutes <= 0:
        raise ValueError(f"resolution åˆ†é˜æ•¸éœ€ > 0ï¼š{resolution}")

    # ä¸€å¤© 1440 åˆ†é˜
    expected = 1440 // minutes
    return expected


def _expand_points_with_fill(
    points: List[Tuple[int, float]],
    expected_points: int,
) -> List[Tuple[int, float]]:
    """
    æ ¹æ“š ENTSO-E çš„ã€Œç›¸åŒåƒ¹æ ¼çœç•¥å¾ŒçºŒé»ã€è¦å‰‡è£œå€¼ã€‚

    - points: å·²æŒ‰ position æ’åºå¥½çš„ (position, price) åˆ—è¡¨ã€‚
    - expected_points: ä¸€å¤©æ‡‰æœ‰çš„é»æ•¸ï¼ˆå¦‚ 24 / 48 / 96ï¼‰ã€‚

    è¦å‰‡ï¼š
    1. è‹¥ position æœ‰è·³è™Ÿï¼Œä¾‹å¦‚ 4 â†’ 6ï¼Œå‰‡è£œä¸Š 5ï¼Œåƒ¹æ ¼ = position 4 çš„åƒ¹æ ¼ã€‚
    2. è‹¥æœ€å¾Œä¸€ç­† position < expected_pointsï¼Œå‰‡å¾ŒçºŒåƒ¹æ ¼çš†æ²¿ç”¨æœ€å¾Œä¸€ç­†åƒ¹æ ¼ã€‚
    """
    if not points:
        return []

    points = sorted(points, key=lambda x: x[0])
    expanded: List[Tuple[int, float]] = []

    prev_pos, prev_price = points[0]
    if prev_pos != 1:
        # ç†è«–ä¸Šç¬¬ä¸€å€‹ position æ‡‰ç‚º 1
        raise ValueError(f"ç¬¬ä¸€å€‹ position ä¸æ˜¯ 1ï¼Œè€Œæ˜¯ {prev_pos}ï¼Œè³‡æ–™æ ¼å¼å¯èƒ½ç•°å¸¸ã€‚")

    expanded.append((prev_pos, prev_price))

    for pos, price in points[1:]:
        if pos <= prev_pos:
            raise ValueError(f"position ééå¢ï¼š{prev_pos} â†’ {pos}")

        # è‹¥ä¸­é–“æœ‰ç¼ºå€¼ï¼Œä¾‹å¦‚ prev_pos=4, pos=6ï¼Œå‰‡è£œä¸Š 5
        if pos > prev_pos + 1:
            for missing_pos in range(prev_pos + 1, pos):
                expanded.append((missing_pos, prev_price))

        expanded.append((pos, price))
        prev_pos, prev_price = pos, price

    # è‹¥æœ€å¾Œä¸€ç­† position é‚„æ²’åˆ° expected_pointsï¼Œè£œåˆ°å°¾ç«¯
    if prev_pos < expected_points:
        for missing_pos in range(prev_pos + 1, expected_points + 1):
            expanded.append((missing_pos, prev_price))

    if prev_pos > expected_points:
        print(
            f"[è­¦å‘Š] æ­¤ TimeSeries æœ€å¾Œ position={prev_pos}ï¼Œ"
            f"å¤§æ–¼é æœŸ {expected_points}ï¼Œè«‹æª¢æŸ¥ resolution é…ç½®ã€‚"
        )

    return expanded


# =========================== #
# 3 ğŸ”¹ å®šç¾©ä¸»åŠŸèƒ½ï¼šXML è½‰ Raw CSV
# =========================== #
def parse_da_xml_to_raw_csv_bytes(
    xml_bytes: bytes,
    country_code: str,
) -> bytes:
    """
    å°‡æ—¥å‰é›»åƒ¹ XML è§£æç‚ºã€ŒåŸå§‹ã€CSVã€‚
    """
    root = ET.fromstring(xml_bytes)
    ns = root.tag.split("}")[0].strip("{")

    rows = []
    seen_delivery_days: Set[date] = set()

    for ts in root.findall(f".//{{{ns}}}TimeSeries"):

        # 3-1 ğŸ”¹ classificationSequence éæ¿¾ (è·³éç‰¹æ®Šåºåˆ—)
        cls_elem = ts.find(
            f"{{{ns}}}classificationSequence_AttributeInstanceComponent.position"
        )
        if cls_elem is not None and cls_elem.text and cls_elem.text.strip():
            cls_val = cls_elem.text.strip()
            print(f"[åˆ†é¡åºè™Ÿ] è·³é TimeSeries (Seq={cls_val})")
            continue

        # 3-2 ğŸ”¹ å–å¾—äº¤å‰²æ—¥ä¸¦å»é‡
        # é€™è£¡ç›´æ¥ä½¿ç”¨ timezone_helperï¼Œç¢ºä¿é‚è¼¯çµ±ä¸€
        delivery_day = get_da_delivery_date_from_timeseries(ts)

        if delivery_day in seen_delivery_days:
            print(
                f"[äº¤å‰²æ—¥å»é‡] è·³é TimeSeriesï¼šäº¤å‰²æ—¥ {delivery_day} å·²å­˜åœ¨æœ‰æ•ˆåºåˆ—ã€‚"
            )
            continue
        else:
            seen_delivery_days.add(delivery_day)

        period = ts.find(f"{{{ns}}}Period")
        if period is None:
            continue

        res_elem = period.find(f"{{{ns}}}resolution")
        if res_elem is None or not (res_elem.text and res_elem.text.strip()):
            raise ValueError("Period ç¼ºå°‘ resolutionã€‚")

        resolution_str = res_elem.text.strip()
        expected_points = _resolution_to_expected_points(resolution_str)

        # 3-3 ğŸ”¹ è§£æ Point ä¸¦ä¾ ENTSO-E è¦å‰‡è£œå€¼
        raw_points: List[Tuple[int, float]] = []
        for pt in period.findall(f"{{{ns}}}Point"):
            pos_elem = pt.find(f"{{{ns}}}position")
            price_elem = pt.find(f"{{{ns}}}price.amount")

            if (
                pos_elem is None
                or not (pos_elem.text and pos_elem.text.strip())
                or price_elem is None
                or not (price_elem.text and price_elem.text.strip())
            ):
                continue

            pos = int(pos_elem.text.strip())
            price = float(price_elem.text.strip())
            raw_points.append((pos, price))

        if not raw_points:
            print("[è­¦å‘Š] æŸä¸€å€‹ TimeSeries å®Œå…¨æ²’æœ‰ Pointï¼Œå·²ç•¥éã€‚")
            continue

        expanded_points = _expand_points_with_fill(raw_points, expected_points)

        date_str = delivery_day.strftime("%Y/%m/%d")

        for pos, price in expanded_points:
            rows.append(
                {
                    "Date": date_str,
                    "Market Time Unit (MTU)": pos,
                    "Day-ahead Price (EUR/MWh)": price,
                }
            )

    # 3-4 ğŸ”¹ è¼¸å‡º Raw CSV Bytes
    if not rows:
        raise ValueError("è§£æå¾Œæ²’æœ‰ä»»ä½•è³‡æ–™ï¼Œè«‹æª¢æŸ¥ XML å…§å®¹ã€‚")

    df = pd.DataFrame(rows)
    df.sort_values(by=["Date", "Market Time Unit (MTU)"], inplace=True)

    buf = io.StringIO()
    df.to_csv(buf, index=False)
    return buf.getvalue().encode("utf-8")


# =========================== #
# 4 ğŸ”¹ å®šç¾©ä¸»åŠŸèƒ½ï¼šRaw CSV è½‰ Hourly CSV
# =========================== #
def convert_raw_mtu_csv_to_hourly_csv_bytes(raw_csv_bytes: bytes) -> bytes:
    """
    å°‡ã€ŒåŸå§‹ MTU CSVã€è½‰æ›ç‚ºã€Œæ¯å°æ™‚ã€CSVã€‚
    æ”¯æ´è§£æåº¦ï¼š60min (24ç­†), 30min (48ç­†), 15min (96ç­†)ã€‚
    """
    # 4-1 ğŸ”¹ è®€å– Raw CSV ä¸¦æª¢æŸ¥æ¬„ä½
    buf = io.StringIO(raw_csv_bytes.decode("utf-8"))
    df = pd.read_csv(buf)

    required_cols = {
        "Date",
        "Market Time Unit (MTU)",
        "Day-ahead Price (EUR/MWh)",
    }
    if not required_cols.issubset(df.columns):
        raise ValueError(
            f"åŸå§‹ CSV æ¬„ä½ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{required_cols - set(df.columns)}"
        )

    # å…è¨±çš„æ¯æ—¥ç­†æ•¸é›†åˆï¼ˆä¾‹å¦‚ 60min -> 24, 30min -> 48, 15min -> 96ï¼‰
    allowed_counts = {int(1440 / m) for m in DA_SUPPORTED_RESOLUTION_MINUTES}

    df = df.copy()
    df.sort_values(by=["Date", "Market Time Unit (MTU)"], inplace=True)

    all_hourly_rows = []

    # 4-2 ğŸ”¹ ä¾ settings è¨­å®šæª¢æŸ¥æ¯æ—¥è³‡æ–™ç­†æ•¸
    for date_value, df_day in df.groupby("Date"):
        df_day = df_day.copy()
        n_points = len(df_day)

        if n_points not in allowed_counts:
            msg = (
                f"æ—¥æœŸ {date_value} çš„ MTU ç­†æ•¸ç‚º {n_points}ï¼Œ"
                f"ä¸ç¬¦åˆç›®å‰æ”¯æ´çš„ç­†æ•¸ {sorted(allowed_counts)}ã€‚"
            )
            if DA_SKIP_UNSUPPORTED_MTU_DAYS:
                print(f"[è­¦å‘Š] {msg} â†’ æš«æ™‚è·³éæ­¤æ—¥ã€‚")
                continue
            else:
                raise ValueError(msg)

        # 4-3 ğŸ”¹ èšåˆé‹ç®— (å¹³å‡å€¼) è½‰æ›ç‚ºå°æ™‚è³‡æ–™
        if n_points == 24:
            # 60 åˆ†é˜è§£æåº¦
            df_day["Hour"] = df_day["Market Time Unit (MTU)"].astype(int)
            hourly = df_day[["Hour", "Day-ahead Price (EUR/MWh)"]].copy()

        elif n_points == 48:
            # 30 åˆ†é˜è§£æåº¦
            df_day["Hour"] = (df_day["Market Time Unit (MTU)"].astype(int) - 1) // 2 + 1
            hourly = (
                df_day.groupby("Hour", as_index=False)["Day-ahead Price (EUR/MWh)"]
                .mean()
            )

        elif n_points == 96:
            # 15 åˆ†é˜è§£æåº¦
            df_day["Hour"] = (df_day["Market Time Unit (MTU)"].astype(int) - 1) // 4 + 1
            hourly = (
                df_day.groupby("Hour", as_index=False)["Day-ahead Price (EUR/MWh)"]
                .mean()
            )
        else:
            # ç†è«–ä¸Šä¸æœƒåŸ·è¡Œåˆ°æ­¤ (å·²ç”± allowed_counts æŠŠé—œ)
            continue

        hourly.sort_values(by="Hour", inplace=True)
        hourly.insert(0, "Date", date_value)
        all_hourly_rows.append(hourly)

    if not all_hourly_rows:
        raise ValueError("è½‰æ›å¾Œæ²’æœ‰ä»»ä½•è³‡æ–™ï¼Œè«‹æª¢æŸ¥åŸå§‹ CSVã€‚")

    # 4-4 ğŸ”¹ è¼¸å‡º Hourly CSV Bytes
    df_hourly = pd.concat(all_hourly_rows, ignore_index=True)
    df_hourly = df_hourly[["Date", "Hour", "Day-ahead Price (EUR/MWh)"]]

    out_buf = io.StringIO()
    df_hourly.to_csv(out_buf, index=False)
    return out_buf.getvalue().encode("utf-8")

# =========================== #
# 5 ğŸ”¹ å®šç¾©é€²éšåˆ†æå·¥å…·
# =========================== #
def calculate_daily_stats(
    hourly_csv_bytes: bytes
) -> Tuple[bytes, dict]:
    """
    è¨ˆç®—æ¯æ—¥çµ±è¨ˆæ•¸æ“šï¼ˆå¹³å‡é›»åƒ¹ã€åƒ¹å·®ã€æ¨™æº–å·®ï¼‰ã€‚
    """
    buf = io.StringIO(hourly_csv_bytes.decode("utf-8"))
    df = pd.read_csv(buf)
    df["Date"] = pd.to_datetime(df["Date"])
    
    # 5-1 ğŸ”¹ åŸ·è¡Œèšåˆé‹ç®— (æ–°å¢ 'std' æ¨™æº–å·®è¨ˆç®—)
    daily_stats = df.groupby("Date")["Day-ahead Price (EUR/MWh)"].agg(
        ["mean", "max", "min", "std"]
    ).reset_index()
    
    daily_stats["Spread"] = daily_stats["max"] - daily_stats["min"]
    
    # é‡æ–°å‘½åæ¬„ä½
    daily_stats.rename(columns={
        "mean": "Daily Average Price",
        "max": "Daily Max Price",
        "min": "Daily Min Price",
        "std": "Daily Volatility (SD)", # æ–°å¢æ³¢å‹•ç‡
        "Spread": "Daily Price Spread"
    }, inplace=True)
    
    daily_stats["Date_Str"] = daily_stats["Date"].dt.strftime("%Y/%m/%d")
    
    # 5-2 ğŸ”¹ è¨ˆç®— UI é¡¯ç¤ºç”¨çš„æ‘˜è¦æ•¸æ“š (Summary Stats)
    if daily_stats.empty:
        raise ValueError("è¨ˆç®—å¾Œçš„çµ±è¨ˆè³‡æ–™ç‚ºç©ºã€‚")

    start_date = daily_stats["Date_Str"].iloc[0]
    end_date = daily_stats["Date_Str"].iloc[-1]
    
    # è¨ˆç®—å„æŒ‡æ¨™çš„å€é–“å¹³å‡
    avg_price = daily_stats["Daily Average Price"].mean()
    avg_spread = daily_stats["Daily Price Spread"].mean()
    avg_volatility = daily_stats["Daily Volatility (SD)"].mean() # æ–°å¢
    
    max_spread_idx = daily_stats["Daily Price Spread"].idxmax()
    max_spread_row = daily_stats.loc[max_spread_idx]
    
    summary = {
        "start_date": start_date,
        "end_date": end_date,
        "avg_price": round(avg_price, 2),   
        "avg_spread": round(avg_spread, 2),
        "avg_volatility": round(avg_volatility, 2), # æ–°å¢
        "max_spread": round(max_spread_row["Daily Price Spread"], 2),
        "max_spread_date": max_spread_row["Date_Str"]
    }
    
    # 5-3 ğŸ”¹ è¼¸å‡º CSV Bytes
    out_df = daily_stats[[
        "Date_Str", 
        "Daily Average Price", 
        "Daily Price Spread", 
        "Daily Volatility (SD)", # åŒ¯å‡ºæª”ä¹ŸåŠ å…¥æ­¤æ¬„
        "Daily Max Price", 
        "Daily Min Price"
    ]].rename(columns={"Date_Str": "Date"})
    
    out_buf = io.StringIO()
    out_df.to_csv(out_buf, index=False, float_format="%.2f")
    
    return out_buf.getvalue().encode("utf-8"), summary